{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb794221-4064-48c8-8d74-c07dcd70400d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " TRAINING WITH *SKLEARN* TOOK 0.01 MINUTES, DETAILS BELOW\n",
      "============================================================\n",
      " Num_rows: 300\n",
      " Num_trees: 100\n",
      " Num_cores: 4\n",
      " Accuracy: 0.04\n",
      "------------------------------------------------------------\n",
      "Done with sklearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/23 10:46:25 WARN Utils: Your hostname, manuemk-Latitude-E5470 resolves to a loopback address: 127.0.1.1; using 10.6.253.186 instead (on interface wlp2s0)\n",
      "25/01/23 10:46:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/23 10:46:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ERROR:root:Exception while sending command.                        (8 + 4) / 31]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=59>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o24.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/manuemk/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o304.evaluate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    200\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 197\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m sdf \u001b[38;5;241m=\u001b[39m sdf\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Run the Random Forest model with Spark MLlib\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mbuild_RF_with_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[1;32m    200\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[0;32mIn[1], line 142\u001b[0m, in \u001b[0;36mbuild_RF_with_spark\u001b[0;34m(sdf, sample_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Evaluate the model using accuracy\u001b[39;00m\n\u001b[1;32m    141\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, metricName\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Count the total rows processed\u001b[39;00m\n\u001b[1;32m    145\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m spark_df\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o304.evaluate"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier  as RF\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Global variables\n",
    "# Sample size\n",
    "SAMPLE_SIZE = 300\n",
    "\n",
    "# Number of trees in the random forest\n",
    "N_TREES = 100\n",
    "\n",
    "# Number of cores to use\n",
    "N_CORES = 4\n",
    "\n",
    "def build_RF_with_sklearn(df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model using scikit-learn, evaluates its performance, and returns the model and metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing features and the target variable. The target column should be named 'hotel_cluster'.\n",
    "    sample_size : int\n",
    "        The number of rows to sample from the input DataFrame. Default is 100,000\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    clf : RandomForestClassifier\n",
    "        The trained Random Forest model.\n",
    "    accuracy : float\n",
    "        The accuracy of the model on the test set.\n",
    "    metric : float\n",
    "        The MAP@5 evaluation score for the model on the test set.\n",
    "    \"\"\"\n",
    "    # sample the data\n",
    "    df = df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Convert all columns to integers\n",
    "    for column in df:\n",
    "        df[column] = df[column].astype(str).astype(int)\n",
    "\n",
    "    # Splitting features and target\n",
    "    X = df.drop(['hotel_cluster'], axis=1)\n",
    "    y = df['hotel_cluster'].values\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Training the Random Forest\n",
    "    start = datetime.now()\n",
    "    clf = RF(n_jobs=N_CORES, n_estimators=N_TREES, random_state=42, \n",
    "             max_depth=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    end = datetime.now()\n",
    "    time_taken = (end - start).total_seconds() / 60\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    \n",
    "    # # Evaluate using MAP@5\n",
    "    # probs = clf.predict_proba(X_test)\n",
    "    # actual = y_test\n",
    "    # predicted = np.argsort(probs, axis=1)[:, -np.arange(5)]\n",
    "    # metric = 0.0\n",
    "    # for i in range(5):\n",
    "    #     metric += np.sum(actual == predicted[:, i]) / (i + 1)\n",
    "    # metric /= actual.shape[0]\n",
    "\n",
    "    # print()\n",
    "    # print(f\"MAP@5: {metric:.4f}\")\n",
    "\n",
    "    num_rows = X.shape[0]\n",
    "    print(\"=\"*60)\n",
    "    print(' TRAINING WITH *SKLEARN* TOOK {:.2f} MINUTES, DETAILS BELOW'.format(time_taken))\n",
    "    print(\"=\"*60)\n",
    "    print(f\" Num_rows: {num_rows:,}\")\n",
    "    print(f\" Num_trees: {N_TREES:,}\")\n",
    "    print(f\" Num_cores: {N_CORES:,}\")\n",
    "    print(f\" Accuracy: {accuracy:,.2f}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "def build_RF_with_spark(sdf, sample_size=100):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model using Spark MLlib, evaluates its performance, and returns the model and metrics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    sdf : pyspark.sql.DataFrame\n",
    "        The input DataFrame containing features and the target variable. The target column should be named 'hotel_cluster'.\n",
    "    Returns:\n",
    "    -------\n",
    "    model : pyspark.ml.classification.RandomForestClassificationModel\n",
    "        The trained Random Forest model.\n",
    "    accuracy : float\n",
    "        The accuracy of the model on the test set.\n",
    "    time_taken : float\n",
    "        The time taken to train the model (in seconds).\n",
    "    \"\"\"\n",
    "    # Sample the data\n",
    "    total_size = 376\n",
    "    sample_fraction = sample_size/total_size\n",
    "    sdf = sdf.sample(False, sample_fraction)\n",
    "    # Prepare the data for Spark MLlib\n",
    "    feature_columns = [col for col in sdf.columns if col != 'hotel_cluster']\n",
    "\n",
    "    # Convert all columns to integers\n",
    "    for column in sdf.columns:\n",
    "        sdf = sdf.withColumn(column, sdf[column].cast(IntegerType()))\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    spark_df = assembler.transform(sdf).select(\"features\", col(\"hotel_cluster\").alias(\"label\"))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_df, test_df = spark_df.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", \n",
    "                                numTrees=N_TREES, maxDepth=4)\n",
    "    start_time = datetime.now()\n",
    "    model = rf.fit(train_df)\n",
    "    end_time = datetime.now()\n",
    "    time_taken = (end_time - start_time).total_seconds()/60\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_df)\n",
    "\n",
    "    # Evaluate the model using accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    # Count the total rows processed\n",
    "    num_rows = spark_df.count()\n",
    "    print(\"=\"*60)\n",
    "    print(' TRAINING WITH *SPARK* TOOK {:.2f} MINUTES, DETAILS BELOW'.format(time_taken))\n",
    "    print(\"=\"*60)\n",
    "    print(f\" Num_rows: {num_rows:,}\")\n",
    "    print(f\" Num_trees: {N_TREES:,}\")\n",
    "    print(f\" Num_cores: {N_CORES:,}\")\n",
    "    print(f\" Accuracy: {accuracy:,.2f}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "def main():\n",
    "    # Load the data\n",
    "    DATA_FILE = Path.cwd().joinpath(\"/home/manuemk/Documents/AIMS/BDA/kaggle-expedia-train.csv\")\n",
    "    # Define the columns to be used\n",
    "    COLS = ['site_name', 'user_location_region', 'is_package', 'srch_adults_cnt', 'srch_children_cnt','srch_destination_id', 'hotel_market', 'hotel_country', 'hotel_cluster']\n",
    "\n",
    "    # =======================================\n",
    "    # RUN WITH SKLEARN\n",
    "    # =======================================\n",
    "    pdf = pd.read_csv(DATA_FILE, usecols=COLS)\n",
    "    pdf = pdf[COLS]\n",
    "    build_RF_with_sklearn(pdf, sample_size=SAMPLE_SIZE)\n",
    "    print(\"Done with sklearn\")\n",
    "    \n",
    "    # =======================================\n",
    "    # RUN WITH SPARK\n",
    "    # =======================================\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder\\\n",
    "                .appName(\"LargeDatasetProcessing\")\\\n",
    "                .master(f\"local[{N_CORES}]\")\\\n",
    "                .config(\"spark.driver.memory\", \"4g\")\\\n",
    "                .config(\"spark.executor.memory\", \"4g\")\\\n",
    "                .config(\"spark.memory.offHeap.enabled\", \"true\")\\\n",
    "                .config(\"spark.sql.shuffle.partitions\", \"8\")\\\n",
    "                .config(\"spark.memory.offHeap.size\", \"1g\")\\\n",
    "                .getOrCreate()\n",
    "\n",
    "    \n",
    "    # Set log level to ERROR\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    # Load and parse the data file, converting it to a DataFrame.\n",
    "    sdf = spark.read.csv(str(DATA_FILE), header=True)\n",
    "\n",
    "    # Select the columns to be used\n",
    "    sdf = sdf.select(COLS)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    sdf = sdf.dropna()\n",
    "    \n",
    "    # Run the Random Forest model with Spark MLlib\n",
    "    build_RF_with_spark(sdf, sample_size=SAMPLE_SIZE)\n",
    "    \n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8acf6-4545-4018-b050-bce8ca41e3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61972dd9-e3ed-4aee-b7e0-6f9479eba452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
